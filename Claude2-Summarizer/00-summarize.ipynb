{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization with Claude-2\n",
    "\n",
    "This notebook leverages the Claude-2 model with a long context window of 100k tokens to tackle long-form summarization of financial documents.\n",
    "\n",
    "##### Table of Contents\n",
    "- Initialization\n",
    "  - Import Libraries\n",
    "  - Logging Setup\n",
    "- Configuration\n",
    "  - Load API Credentials\n",
    "  - Client and Encoder Initialization\n",
    "- Preprocessing\n",
    "  - Token Calculation Helpers\n",
    "  - Load PDF Content\n",
    "  - Segment PDF into Chunks\n",
    "- Summarization\n",
    "  - Generate Summaries for Chunks\n",
    "  - Consolidate and Refine Summaries\n",
    "- Output\n",
    "  - Save Final Summary\n",
    "  - Save Individual Summaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import HUMAN_PROMPT\n",
    "from anthropic import AI_PROMPT\n",
    "from anthropic import Client\n",
    "from typing import List\n",
    "import tiktoken\n",
    "import logging\n",
    "import yaml \n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./../credentials/claude-api.yml', 'rb') as f:\n",
    "    credentials = yaml.safe_load(f)\n",
    "    \n",
    "api_key = credentials['key']\n",
    "os.environ['ANTHROPIC_API_KEY'] = api_key\n",
    "\n",
    "client = Client(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODING_NAME = 'cl100k_base'\n",
    "encoder = tiktoken.get_encoding(ENCODING_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./DATA/file-2.txt', 'r') as f:\n",
    "    pdf = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_tokens(contexts: list) -> int:\n",
    "    total_tokens = 0\n",
    "    for context in contexts:\n",
    "        n_tokens = len(encoder.encode(context))\n",
    "        total_tokens += n_tokens \n",
    "    return total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Approximate number of tokens in the PDF = 414095\n"
     ]
    }
   ],
   "source": [
    "total_tokens = get_total_tokens([pdf])\n",
    "logger.info(f'Approximate number of tokens in the PDF = {total_tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segment the PDF into N chunks of length ~100k each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(text: str, c: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split the input text into chunks where each chunk contains approximately c tokens.\n",
    "    \n",
    "    Parameters:\n",
    "    - text (str): The input text to be chunked.\n",
    "    - c (int): The approximate number of tokens per chunk.\n",
    "    \n",
    "    Returns:\n",
    "    - List[str]: A list of string chunks.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_token_count = 0\n",
    "\n",
    "    # Iterate over tokens in the text\n",
    "    for token in encoder.encode(text):\n",
    "        # Update the current chunk and token count\n",
    "        current_chunk += encoder.decode([token])\n",
    "        current_token_count += 1\n",
    "\n",
    "        # If the current token count reaches c, add the chunk to the list and reset\n",
    "        if current_token_count >= c:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = \"\"\n",
    "            current_token_count = 0\n",
    "\n",
    "    # Add the last chunk if it's non-empty\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "    \n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "CLAUDE_CONTEXT_WINDOW = 100000\n",
    "MAX_OUTPUT_TOKENS = 8192\n",
    "c = CLAUDE_CONTEXT_WINDOW - MAX_OUTPUT_TOKENS\n",
    "chunks = split_into_chunks(pdf, c)\n",
    "logger.info(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens in chunk 0 = 91757\n",
      "Number of tokens in chunk 4 = 46860\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'Number of tokens in chunk 0 = {get_total_tokens([chunks[0]])}')\n",
    "logger.info(f'Number of tokens in chunk 4 = {get_total_tokens([chunks[4]])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Human:\n"
     ]
    }
   ],
   "source": [
    "logger.info(HUMAN_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(AI_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35.9 ms, sys: 9.29 ms, total: 45.1 ms\n",
      "Wall time: 6min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "summaries = []\n",
    "\n",
    "for chunk in chunks:\n",
    "    prompt = f\"\"\"{HUMAN_PROMPT} You are a Financial Regulations & Derivatives Expert. Given the chunk below, extract all the proposed changes related to `the processing of derivative contracts` into a long detailed summary with bullet points.\\n\\n{chunk}\\n\\n{AI_PROMPT}\"\"\"\n",
    "    response = client.completions.create(prompt=prompt, \n",
    "                             model='claude-2', \n",
    "                             max_tokens_to_sample=MAX_OUTPUT_TOKENS)\n",
    "    summary = response.completion\n",
    "    summaries.append(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consolidate and refine generated summaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Here is a summary of the key proposed changes related to the processing of derivative contracts:\n",
      "\n",
      "- Require banks to use the standardized approach for counterparty credit risk (SA-CCR) to calculate exposure amounts for all derivative contracts. This replaces internal models and aims to standardize calculations across banks.\n",
      "\n",
      "- Make technical revisions to SA-CCR to improve implementation consistency. This includes revisions to the treatment of collateral, supervisory delta adjustments, decomposition of indices, etc. \n",
      "\n",
      "- Introduce minimum haircut floors for certain transactions with unregulated entities to limit leverage build up outside the banking system. The floors are based on collateral type.\n",
      "\n",
      "- Replace model-based approaches for credit risk mitigation with standardized approaches from the current framework. This prohibits recognition of certain credit derivatives. \n",
      "\n",
      "- Make revisions to the securitization framework, including changes to the standardized approach, treatment of overlapping exposures, restrictions on capital relief, and new frameworks for NPL securitizations.\n",
      "\n",
      "- Introduce new requirements for calculating CVA risk capital requirements using the basic CVA approach or standardized CVA approach. This aims to better capture CVA risk and recognize hedges. \n",
      "\n",
      "- Allow supervisors to apply multipliers to increase capital requirements if a bank's CVA risk is deemed insufficiently capitalized. \n",
      "\n",
      "Overall, the changes aim to standardize, simplify and strengthen capital requirements related to derivative contracts and securitizations, while improving risk sensitivity.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.49 ms, sys: 3.13 ms, total: 10.6 ms\n",
      "Wall time: 21.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "stacked_summaries = '\\n'.join(summaries)\n",
    "\n",
    "prompt = f\"\"\"{HUMAN_PROMPT} You are a Financial Regulations & Derivatives Expert. Given context below, create a detailed summary broken down by sections.\\n\\n{stacked_summaries}\\n\\n{AI_PROMPT}\"\"\"\n",
    "response = client.completions.create(prompt=prompt, \n",
    "                            model='claude-2', \n",
    "                            max_tokens_to_sample=MAX_OUTPUT_TOKENS)\n",
    "final_summary = response.completion\n",
    "logger.info(final_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./DATA/final-summary.txt', 'w') as f:\n",
    "    f.write(final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, summary in enumerate(summaries):\n",
    "    i += 1\n",
    "    with open(f'./DATA/summary-{i}.txt', 'w') as f:\n",
    "        f.write(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".citi-icg-pocs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
