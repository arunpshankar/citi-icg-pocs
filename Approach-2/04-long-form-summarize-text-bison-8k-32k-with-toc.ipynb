{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Form Summarization using `text-bison 8k`, `32k` and `DocAI`\n",
    "\n",
    "ℹ️ **Note:** This leverages the table of contents (TOC) from the documents.\n",
    "\n",
    "<span style=\"color:blue; font-weight:bold\">Setup</span>\n",
    "- _Import Libraries_: Get all the necessary Python libraries onboard.\n",
    "- _Initialize Logging_: Make sure we have a log of all the operations.\n",
    "\n",
    "<span style=\"color:green; font-weight:bold\">Configuration</span>\n",
    "- _Define Parameters_: Lay out all the essential parameters for our operations.\n",
    "- _Authentication_: Authenticate the user and ensure security.\n",
    "- _Setup Clients_: Initialize the API clients for our services.\n",
    "\n",
    "<span style=\"color:purple; font-weight:bold\">Document Preprocessing</span>\n",
    "- _Split PDF_: Break the PDF into manageable chunks for better OCR results.\n",
    "\n",
    "<span style=\"color:orange; font-weight:bold\">Document OCR</span>\n",
    "- _Convert to Text_: Use DocAI to transcribe the PDF content into text format.\n",
    "\n",
    "<span style=\"color:red; font-weight:bold\">Summarization</span>\n",
    "- _Map Reduce 1 uses `text-bison-8k`_\n",
    "- _Map Reduce 2 and 3 use `text-bison-32k`_\n",
    "\n",
    "<span style=\"color:teal; font-weight:bold\">Final Consolidation</span>\n",
    "- _Refine Summary_: Make sure the summary is crisp and to the point.\n",
    "- _Save_: Store the final summary for future reference.\n",
    "\n",
    "<span style=\"color:brown; font-weight:bold\">Focused Summarization</span>\n",
    "- _Extract Sections_: Dive deeper and pull out specific sections from the final summary for a more focused understanding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.preview.language_models import TextGenerationModel\n",
    "from google.api_core.client_options import ClientOptions\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from google.cloud import documentai\n",
    "from pypdf import PdfWriter\n",
    "from pypdf import PdfReader \n",
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "import vertexai\n",
    "import requests\n",
    "import logging\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup logging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Essentials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = './../credentials/vai-key.json'\n",
    "access_token = !gcloud auth print-access-token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'arun-genai-bb'\n",
    "LOCATION = 'us-central1'\n",
    "MODEL_NAME_8K = 'text-bison@latest'\n",
    "MODEL_NAME_32K = 'text-bison-32k@latest'\n",
    "# May need to request quota upgrade https://console.cloud.google.com/iam-admin/quotas?project=vertex-pe-only Service: Vertex AI API, text-bison-32k region:us-central1\n",
    "ENCODING_NAME = 'cl100k_base'\n",
    "CONTEXT_LENGTH = 32000  # text-bison-32k\n",
    "STREAMING_API_URL_8K = f'https://us-central1-aiplatform.googleapis.com/ui/projects/{PROJECT_ID}/locations/us-central1/publishers/google/models/{MODEL_NAME_8K}:serverStreamingPredict'\n",
    "STREAMING_API_URL_32K = f'https://us-central1-aiplatform.googleapis.com/ui/projects/{PROJECT_ID}/locations/us-central1/publishers/google/models/{MODEL_NAME_32K}:serverStreamingPredict'\n",
    "DOCAI_PROCESSOR_NAME = 'projects/390991481152/locations/us/processors/ad9557a5be49204e'  # copy from notebook 00\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_options = ClientOptions(api_endpoint=f'us-documentai.googleapis.com')\n",
    "docai_client = documentai.DocumentProcessorServiceClient(client_options=client_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using encoder==cl100k_base\n"
     ]
    }
   ],
   "source": [
    "encoder = tiktoken.get_encoding(ENCODING_NAME)\n",
    "logger.info(f'Using encoder=={encoder.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Google DocumentAI to process input PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Break PDF into smaller PDFs for OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_INPUT_DIR = './DATA/INPUT'\n",
    "LOCAL_OUTPUT_DIR = './DATA/OUTPUT'\n",
    "FILE_NAME = 'file-2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(f'{LOCAL_INPUT_DIR}/{FILE_NAME}.pdf')\n",
    "pages = {}\n",
    "\n",
    "for i, page in enumerate(reader.pages):\n",
    "    pages[i] = page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(reader.pages)\n",
    "d = 15  # docai has a current constraint of 15 pages per document \n",
    "\n",
    "for i in range(0, n, d):\n",
    "    writer = PdfWriter()\n",
    "    for j in range(i, i+d):\n",
    "        if j < n:\n",
    "            writer.add_page(pages[j])\n",
    "    os.makedirs(f'{LOCAL_INPUT_DIR}/{FILE_NAME}/PARTS/', exist_ok=True)\n",
    "    with open(f'{LOCAL_INPUT_DIR}/{FILE_NAME}/PARTS/{FILE_NAME}_{i+1}-{i+d}.pdf', 'wb') as f:\n",
    "        writer.write(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layout_to_text(layout: documentai.Document.Page.Layout, text: str) -> str:\n",
    "    \"\"\"\n",
    "    Document AI identifies text in different parts of the document by their\n",
    "    offsets in the entirety of the document's text. This function converts\n",
    "    offsets to a string.\n",
    "    \"\"\"\n",
    "    # If a text segment spans several lines, it will be stored in different text segments.\n",
    "    return ''.join(text[int(segment.start_index): int(segment.end_index)] for segment in layout.text_anchor.text_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_paths(dir_name: str) -> list:\n",
    "    file_paths = []\n",
    "    for file_name in os.listdir(dir_name):\n",
    "        if os.path.isfile(os.path.join(dir_name, file_name)):\n",
    "            file_path = os.path.join(dir_name, file_name)\n",
    "            file_paths.append(file_path)\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr_docai(file_path: str) -> dict:\n",
    "    pages_map = {}\n",
    "\n",
    "    with open(file_path, 'rb') as f:\n",
    "        pdf = f.read()\n",
    "        raw_document = documentai.RawDocument(content=pdf, mime_type='application/pdf')\n",
    "        request = documentai.ProcessRequest(name=DOCAI_PROCESSOR_NAME, raw_document=raw_document)\n",
    "        response = docai_client.process_document(request=request)\n",
    "        text = response.document.text\n",
    "        file_name = file_path.split('/')[-1]\n",
    "        page_number = int(file_name.split('.')[0].split('-')[-1])\n",
    "        for page in response.document.pages:\n",
    "            page_text = []\n",
    "            for paragraph in page.paragraphs:\n",
    "                paragraph_text = layout_to_text(paragraph.layout, text)\n",
    "                page_text.append(paragraph_text)\n",
    "            pages_map[page_number] = ''.join(page_text)\n",
    "            page_number += 1\n",
    "    return pages_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73it [00:48,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.8 s, sys: 3.13 s, total: 6.93 s\n",
      "Wall time: 49 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "input_dir = f'./DATA/INPUT/{FILE_NAME}/PARTS/'\n",
    "file_paths = get_file_paths(input_dir)\n",
    "    \n",
    "pages_map_list = []\n",
    "with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:  \n",
    "    pages_map_list = list(tqdm(executor.map(ocr_docai, file_paths)))\n",
    "\n",
    "merged_dict = {k: v for d in pages_map_list for k, v in d.items()}   \n",
    "sorted_pages_map = dict(sorted(merged_dict.items()))\n",
    "\n",
    "pages = []\n",
    "for _, page_text in sorted_pages_map.items():\n",
    "    pages.append(page_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save concatenated pages as txt for later use (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_pages = ''.join(pages)\n",
    "os.makedirs(f'{LOCAL_OUTPUT_DIR}/{FILE_NAME}/VAI/', exist_ok=True)\n",
    "with open(f'{LOCAL_OUTPUT_DIR}/{FILE_NAME}/VAI/{FILE_NAME}.txt', 'w') as out:\n",
    "    out.write(extracted_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_tokens(contexts: list) -> int:\n",
    "    total_tokens = 0\n",
    "    for context in contexts:\n",
    "        n_tokens = len(encoder.encode(context))\n",
    "        total_tokens += n_tokens \n",
    "    return total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total tokens in the input doc = 414765\n"
     ]
    }
   ],
   "source": [
    "total_tokens = get_total_tokens([extracted_pages])\n",
    "logger.info(f'Total tokens in the input doc = {total_tokens}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_tokens_per_page(contexts: list) -> list:\n",
    "    max_tokens_per_page = 0\n",
    "    for context in contexts:\n",
    "        n_tokens = len(encoder.encode(context))\n",
    "        if n_tokens > max_tokens_per_page:\n",
    "            max_tokens_per_page = n_tokens\n",
    "    return max_tokens_per_page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map Reduce 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_via_streaming_api_mr1(chunk: str) -> str:\n",
    "    prompt = f'You are a Financial Regulations & Derivatives Expert. Summarize the following information a minimum set of unordered key bullet points in English, capturing the essential details.\\n\\n{chunk}'\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {access_token[0]}\",\n",
    "        \"Content-Type\": \"application/json; charset=utf-8\"\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"inputs\": [\n",
    "            {\n",
    "                \"struct_val\": {\n",
    "                    \"prompt\": {\n",
    "                        \"string_val\": [prompt]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"parameters\": {\n",
    "            \"struct_val\": {\n",
    "                \"temperature\": {\"float_val\": 0.0},\n",
    "                \"maxOutputTokens\": {\"int_val\": 256},\n",
    "                \"topK\": {\"int_val\": 40},\n",
    "                \"topP\": {\"float_val\": 0.8}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    response = requests.post(STREAMING_API_URL_8K, headers=headers, json=data)\n",
    "    content = json.loads(response.content)\n",
    "    output = []\n",
    "\n",
    "    for item in content:\n",
    "        try:\n",
    "            text = item['outputs'][0]['structVal']['content']['stringVal'][0]\n",
    "            output.append(text)\n",
    "        except Exception as e:\n",
    "            logger.error(f'Content error => {content}')\n",
    "    output = ''.join(output)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXTS_PER_CALL = 5  # process 5 pages per API call\n",
    "MAX_OUTPUT_TOKENS = 256\n",
    "\n",
    "def reduce_mr1(contexts: list) -> list:\n",
    "    partitions = []\n",
    "    max_input_tokens = CONTEXT_LENGTH - MAX_OUTPUT_TOKENS\n",
    "    logger.info(f'Max input tokens allowed per API call = {max_input_tokens}')\n",
    "    max_tokens_per_page = get_max_tokens_per_page(contexts)\n",
    "    logger.info(f'Max tokens per page = {max_tokens_per_page}')\n",
    "    logger.info(f'Processing {CONTEXTS_PER_CALL} pages per API call')\n",
    "    \n",
    "    for i in range(0, len(contexts), CONTEXTS_PER_CALL):\n",
    "        partitions.append(contexts[i: i+CONTEXTS_PER_CALL])\n",
    "\n",
    "    chunks = []\n",
    "    for partition in partitions:\n",
    "        chunks.append('\\n'.join(partition))\n",
    "\n",
    "    reduced_contexts = []\n",
    "\n",
    "    # max_workers can result in running over quota limits for invocation | current limit for text bison is 60/min\n",
    "    # for our experiments, we set max_workers=4 cores without any limit breach\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:  \n",
    "        reduced_contexts = list(tqdm(executor.map(get_summary_via_streaming_api_mr1, chunks),  total=len(chunks)))\n",
    "    return reduced_contexts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of pages to process = 1089\n",
      "Max input tokens allowed per API call = 31744\n",
      "Max tokens per page = 722\n",
      "Processing 5 pages per API call\n",
      " 20%|██        | 44/218 [01:03<04:26,  1.53s/it]"
     ]
    }
   ],
   "source": [
    "logger.info(f'Number of pages to process = {len(pages)}')\n",
    "summaries = reduce_mr1(pages)\n",
    "logger.info(f'Number of generated summaries = {len(summaries)}')\n",
    "n_tokens = get_total_tokens(summaries)\n",
    "logger.info(f'Total number of tokens in generated summaries = {n_tokens}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(summaries[5])\n",
    "logger.info('-' * 100)\n",
    "logger.info(summaries[15])\n",
    "logger.info('-' * 100)\n",
    "logger.info(summaries[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Persist internediate summaries (Map Reduce 1) to local disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f'Total number of summaries = {len(summaries)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, summary in enumerate(summaries):\n",
    "    os.makedirs(f'{LOCAL_OUTPUT_DIR}/{FILE_NAME}/VAI/MAP_REDUCE_1/', exist_ok=True)\n",
    "    with open(f'{LOCAL_OUTPUT_DIR}/{FILE_NAME}/VAI/MAP_REDUCE_1/summary-{i}.txt', 'w') as f:\n",
    "        f.write(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map Reduce 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Injecting **table of content** into the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc = \"I. Introduction\\nA. Overview of the Proposal\\nB. Use of Internal Models Under the\\nProposed Framework\\nII. Scope of Application\\nIII. Proposed Changes to the Capital Rule\\nA. Calculation of Capital Ratios and\\nApplication of Buffer Requirements\\n1. Standardized Output Floor\\n2. Stress Capital Buffer Requirement\\nB. Definition of Capital\\n1. Accumulated Other Comprehensive\\nIncome\\n2. Regulatory Capital Deductions\\n3. Additional Definition of Capital\\nAdjustments\\n4. Changes to the Definition of Tier 2\\nCapital Applicable to Large Banking\\nOrganizations\\nC. Credit Risk\\n1. Due Diligence\\n2. Proposed Risk Weights for Credit Risk\\n3. Off-Balance Sheet Exposures\\n4. Derivatives\\n5. Credit Risk Mitigation\\nD. Securitization Framework\\n1. Operational Requirements\\n2. Securitization Standardized Approach\\n(SEC–SA)\\n3. Exceptions to the SEC–SA Risk-Based\\nCapital Treatment for Securitization\\nExposures\\n4. Credit Risk Mitigation for Securitization\\nExposures\\nE. Equity Exposures\\n1. Risk-Weighted Asset Amount\\nF. Operational Risk\\n1. Business Indicator\\n2. Business Indicator Component\\n3. Internal Loss Multiplier\\n4. Operational Risk Management and Data\\nCollection Requirements\\nG. Disclosure Requirements\\n1. Proposed Disclosure Requirements\\n2. Specific Public Disclosure Requirements\\nH. Market Risk\\n1. Background\\n2. Scope and Application of the Proposed\\nRule\\n3. Market Risk Covered Position\\n4. Internal Risk Transfers\\n5. General Requirements for Market Risk\\n6. Measure for Market Risk\\n7. Standardized Measure for Market Risk\\n8. Models-Based Measure for Market Risk\\n9. Treatment of Certain Market Risk\\nCovered Positions\\n10. Reporting and Disclosure Requirements\\n11. Technical Amendments\\nI. Credit Valuation Adjustment Risk\\n1. Background\\n2. Scope of Application\\n3. CVA Risk Covered Positions and CVA\\nHedges\\n4. General Risk Management Requirements\\n5. Measure for CVA Risk\\nIV. Transition Provisions\\nA. Transitions for Expanded Total RiskWeighted Assets\\nB. AOCI Regulatory Capital Adjustments\\nV. Impact and Economic Analysis\\nA. Scope and Data\\nB. Impact on Risk-Weighted Assets and\\nCapital Requirements\\nC. Economic Impact on Lending Activity\\nD. Economic Impact on Trading Activity\\nE. Additional Impact Considerations\\nVI. Technical Amendments to the Capital\\nRule\\nA. Additional OCC Technical Amendments\\nB. Additional FDIC Technical\\nAmendments\\nVII. Proposed Amendments to Related Rules\\nand Related Proposals\\nA. OCC Amendments\\nB. Board Amendments\\nC. Related Proposals\\nVIII. Administrative Law Matters\\nA. Paperwork Reduction Act\\nB. Regulatory Flexibility Act\\nC. Plain Language\\nD. Riegle Community Development and\\nRegulatory Improvement Act of 1994\\nE. OCC Unfunded Mandates Reform Act of\\n1995 Determination\\nF. Providing Accountability Through\\nTransparency Act of 2023\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_via_streaming_api_mr2(context: str) -> str:\n",
    "    prompt = f\"\"\"For the context below, create a short summary with the most important bullet points only.\\n\\n{context}\\n\\nDo not repeat bullet points. Assign summaries into the following outline:\\n{toc}. Make it crisp and concise.\"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {access_token[0]}\",\n",
    "        \"Content-Type\": \"application/json; charset=utf-8\"\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"inputs\": [\n",
    "            {\n",
    "                \"struct_val\": {\n",
    "                    \"prompt\": {\n",
    "                        \"string_val\": [prompt]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"parameters\": {\n",
    "            \"struct_val\": {\n",
    "                \"temperature\": {\"float_val\": 0.0},\n",
    "                \"maxOutputTokens\": {\"int_val\": 4096},\n",
    "                \"topK\": {\"int_val\": 40},\n",
    "                \"topP\": {\"float_val\": 0.8}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    response = requests.post(STREAMING_API_URL_32K, headers=headers, json=data)\n",
    "    content = json.loads(response.content)\n",
    "    output = []\n",
    "\n",
    "    for item in content:\n",
    "        try:\n",
    "            text = item['outputs'][0]['structVal']['content']['stringVal'][0]\n",
    "            output.append(text)\n",
    "        except Exception as e:\n",
    "            logger.error(f'Content error => {content}')\n",
    "    output = ''.join(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXTS_PER_CALL = 50  # process 50 summaries per API call\n",
    "\n",
    "def reduce_mr2(contexts: list) -> list:\n",
    "    partitions = []\n",
    "    max_input_tokens = CONTEXT_LENGTH - MAX_OUTPUT_TOKENS\n",
    "    logger.info(f'Max input tokens allowed per API call = {max_input_tokens}')\n",
    "    max_tokens_per_page = get_max_tokens_per_page(contexts)\n",
    "    logger.info(f'Max tokens per page = {max_tokens_per_page}')\n",
    "    logger.info(f'Processing {CONTEXTS_PER_CALL} pages per API call')\n",
    "    \n",
    "    for i in range(0, len(contexts), CONTEXTS_PER_CALL):\n",
    "        partitions.append(contexts[i: i+CONTEXTS_PER_CALL])\n",
    "\n",
    "    chunks = []\n",
    "    for partition in partitions:\n",
    "        chunks.append('\\n'.join(partition))\n",
    "    logger.info(f'Total number of chunks of summaries = {len(chunks)}')\n",
    "\n",
    "    reduced_contexts = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=1) as executor:  \n",
    "        reduced_contexts = list(tqdm(executor.map(get_summary_via_streaming_api_mr2, chunks),  total=len(chunks)))\n",
    "    return reduced_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_summaries = reduce_mr2(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(reduced_summaries[0])\n",
    "logger.info('-' * 100)\n",
    "logger.info(reduced_summaries[1])\n",
    "logger.info('-' * 100)\n",
    "logger.info(reduced_summaries[2])\n",
    "logger.info('-' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Persist internediate summaries (Map Reduce 2) to local disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f'Total number of summaries after map reduce 2 = {len(reduced_summaries)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, summary in enumerate(reduced_summaries):\n",
    "    os.makedirs(f'{LOCAL_OUTPUT_DIR}/{FILE_NAME}/VAI/MAP_REDUCE_2/', exist_ok=True)\n",
    "    with open(f'{LOCAL_OUTPUT_DIR}/{FILE_NAME}/VAI/MAP_REDUCE_2/summary-{i}.txt', 'w') as f:\n",
    "        f.write(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated_summaries = '\\n'.join(reduced_summaries)\n",
    "logger.info(get_total_tokens([consolidated_summaries]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(consolidated_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Consolidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc = \"I. Introduction\\nA. Overview of the Proposal\\nB. Use of Internal Models Under the\\nProposed Framework\\nII. Scope of Application\\nIII. Proposed Changes to the Capital Rule\\nA. Calculation of Capital Ratios and\\nApplication of Buffer Requirements\\n1. Standardized Output Floor\\n2. Stress Capital Buffer Requirement\\nB. Definition of Capital\\n1. Accumulated Other Comprehensive\\nIncome\\n2. Regulatory Capital Deductions\\n3. Additional Definition of Capital\\nAdjustments\\n4. Changes to the Definition of Tier 2\\nCapital Applicable to Large Banking\\nOrganizations\\nC. Credit Risk\\n1. Due Diligence\\n2. Proposed Risk Weights for Credit Risk\\n3. Off-Balance Sheet Exposures\\n4. Derivatives\\n5. Credit Risk Mitigation\\nD. Securitization Framework\\n1. Operational Requirements\\n2. Securitization Standardized Approach\\n(SEC–SA)\\n3. Exceptions to the SEC–SA Risk-Based\\nCapital Treatment for Securitization\\nExposures\\n4. Credit Risk Mitigation for Securitization\\nExposures\\nE. Equity Exposures\\n1. Risk-Weighted Asset Amount\\nF. Operational Risk\\n1. Business Indicator\\n2. Business Indicator Component\\n3. Internal Loss Multiplier\\n4. Operational Risk Management and Data\\nCollection Requirements\\nG. Disclosure Requirements\\n1. Proposed Disclosure Requirements\\n2. Specific Public Disclosure Requirements\\nH. Market Risk\\n1. Background\\n2. Scope and Application of the Proposed\\nRule\\n3. Market Risk Covered Position\\n4. Internal Risk Transfers\\n5. General Requirements for Market Risk\\n6. Measure for Market Risk\\n7. Standardized Measure for Market Risk\\n8. Models-Based Measure for Market Risk\\n9. Treatment of Certain Market Risk\\nCovered Positions\\n10. Reporting and Disclosure Requirements\\n11. Technical Amendments\\nI. Credit Valuation Adjustment Risk\\n1. Background\\n2. Scope of Application\\n3. CVA Risk Covered Positions and CVA\\nHedges\\n4. General Risk Management Requirements\\n5. Measure for CVA Risk\\nIV. Transition Provisions\\nA. Transitions for Expanded Total RiskWeighted Assets\\nB. AOCI Regulatory Capital Adjustments\\nV. Impact and Economic Analysis\\nA. Scope and Data\\nB. Impact on Risk-Weighted Assets and\\nCapital Requirements\\nC. Economic Impact on Lending Activity\\nD. Economic Impact on Trading Activity\\nE. Additional Impact Considerations\\nVI. Technical Amendments to the Capital\\nRule\\nA. Additional OCC Technical Amendments\\nB. Additional FDIC Technical\\nAmendments\\nVII. Proposed Amendments to Related Rules\\nand Related Proposals\\nA. OCC Amendments\\nB. Board Amendments\\nC. Related Proposals\\nVIII. Administrative Law Matters\\nA. Paperwork Reduction Act\\nB. Regulatory Flexibility Act\\nC. Plain Language\\nD. Riegle Community Development and\\nRegulatory Improvement Act of 1994\\nE. OCC Unfunded Mandates Reform Act of\\n1995 Determination\\nF. Providing Accountability Through\\nTransparency Act of 2023\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_via_streaming_api_mr3(context: str) -> str:\n",
    "    prompt = f\"\"\"For the context below, group and consolidate content into abstracts within each applicable section.\\n\\n{context}\\n\\n.\"\"\"\n",
    "    #prompt = f\"\"\"Given the context below, combine and merge duplicate sections and pointers into the sections {toc}.\\n\\n{context}\\nAdd SECTIONS and bullets wherever needed. Clean rewrite and re-number sections.\"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {access_token[0]}\",\n",
    "        \"Content-Type\": \"application/json; charset=utf-8\"\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"inputs\": [\n",
    "            {\n",
    "                \"struct_val\": {\n",
    "                    \"prompt\": {\n",
    "                        \"string_val\": [prompt]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"parameters\": {\n",
    "            \"struct_val\": {\n",
    "                \"temperature\": {\"float_val\": 0.0},\n",
    "                \"maxOutputTokens\": {\"int_val\": 8192},\n",
    "                \"topK\": {\"int_val\": 40},\n",
    "                \"topP\": {\"float_val\": 0.8}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    response = requests.post(STREAMING_API_URL_32K, headers=headers, json=data)\n",
    "    content = json.loads(response.content)\n",
    "    output = []\n",
    "\n",
    "    for item in content:\n",
    "        try:\n",
    "            text = item['outputs'][0]['structVal']['content']['stringVal'][0]\n",
    "            output.append(text)\n",
    "        except Exception as e:\n",
    "            logger.error(f'Content error => {content}')\n",
    "    output = ''.join(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_summary = get_summary_via_streaming_api_mr3(consolidated_summaries)\n",
    "logger.info(final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_OUTPUT_TOKENS = 8192\n",
    "max_input_tokens = CONTEXT_LENGTH - MAX_OUTPUT_TOKENS\n",
    "logger.info(f'Max input tokens allowed per API call = {max_input_tokens}')\n",
    "logger.info(f'Total tokens in final summary = {get_total_tokens([final_summary])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Persist final summary to local disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{LOCAL_OUTPUT_DIR}/{FILE_NAME}/VAI/final-summary.txt', 'w') as f:\n",
    "    f.write(final_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a filtered summary with all the proposed changes on the `Processing of Derivative Contracts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_via_streaming_api_dc(context: str) -> str:\n",
    "    prompt = f\"\"\"\"Given the context below, extract information only related processing of derivative contracts.\\n\\n{context}\\n. Do not in include any other information.\"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {access_token[0]}\",\n",
    "        \"Content-Type\": \"application/json; charset=utf-8\"\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"inputs\": [\n",
    "            {\n",
    "                \"struct_val\": {\n",
    "                    \"prompt\": {\n",
    "                        \"string_val\": [prompt]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"parameters\": {\n",
    "            \"struct_val\": {\n",
    "                \"temperature\": {\"float_val\": 0.0},\n",
    "                \"maxOutputTokens\": {\"int_val\": 8192},\n",
    "                \"topK\": {\"int_val\": 40},\n",
    "                \"topP\": {\"float_val\": 0.8}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    response = requests.post(STREAMING_API_URL_32K, headers=headers, json=data)\n",
    "    content = json.loads(response.content)\n",
    "    output = []\n",
    "\n",
    "    for item in content:\n",
    "        try:\n",
    "            text = item['outputs'][0]['structVal']['content']['stringVal'][0]\n",
    "            output.append(text)\n",
    "        except Exception as e:\n",
    "            logger.error(f'Content error => {content}')\n",
    "    output = ''.join(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposed_changes_summary = get_summary_via_streaming_api_dc(consolidated_summaries)\n",
    "logger.info(proposed_changes_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_OUTPUT_TOKENS = 8192\n",
    "max_input_tokens = CONTEXT_LENGTH - MAX_OUTPUT_TOKENS\n",
    "logger.info(f'Max input tokens allowed per API call = {max_input_tokens}')\n",
    "logger.info(f'Total tokens in final summary = {get_total_tokens([proposed_changes_summary])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{LOCAL_OUTPUT_DIR}/{FILE_NAME}/VAI/proposed-changes-summary.txt', 'w') as f:\n",
    "    f.write(proposed_changes_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
