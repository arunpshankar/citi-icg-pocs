{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Form Summarization using `text-bison-32k` and `DocAI`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.preview.language_models import TextGenerationModel\n",
    "from google.api_core.client_options import ClientOptions\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from google.cloud import documentai\n",
    "from pypdf import PdfWriter\n",
    "from pypdf import PdfReader \n",
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "import vertexai\n",
    "import requests\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "import yaml\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup logging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Essentials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = './../credentials/vai-key.json'\n",
    "access_token = !gcloud auth print-access-token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'arun-genai-bb'\n",
    "LOCATION = 'us-central1'\n",
    "MODEL_NAME = 'text-bison-32k@latest'\n",
    "ENCODING_NAME = 'cl100k_base'\n",
    "CONTEXT_LENGTH = 32000  # text-bison-32k\n",
    "STREAMING_API_URL = f'https://us-central1-aiplatform.googleapis.com/ui/projects/{PROJECT_ID}/locations/us-central1/publishers/google/models/{MODEL_NAME}:serverStreamingPredict'\n",
    "DOCAI_PROCESSOR_NAME = 'projects/390991481152/locations/us/processors/ad9557a5be49204e'  # copy from notebook 00\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_options = ClientOptions(api_endpoint=f'us-documentai.googleapis.com')\n",
    "docai_client = documentai.DocumentProcessorServiceClient(client_options=client_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using encoder==cl100k_base\n"
     ]
    }
   ],
   "source": [
    "encoder = tiktoken.get_encoding(ENCODING_NAME)\n",
    "logger.info(f'Using encoder=={encoder.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using model==text-bison-32k@latest\n"
     ]
    }
   ],
   "source": [
    "model = TextGenerationModel.from_pretrained(MODEL_NAME)\n",
    "logger.info(f'Using model=={model._model_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Google DocumentAI to process input PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Break PDF into smaller PDFs for OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_INPUT_DIR = './DATA/INPUT'\n",
    "LOCAL_OUTPUT_DIR = './DATA/OUTPUT'\n",
    "FILE_NAME = 'file-2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(f'{LOCAL_INPUT_DIR}/{FILE_NAME}.pdf') \n",
    "pages = {}\n",
    "\n",
    "for i, page in enumerate(reader.pages):\n",
    "    pages[i] = page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(reader.pages)\n",
    "d = 15  # docai has a current constraint of 15 pages per document \n",
    "for i in range(0, n, d):\n",
    "    writer = PdfWriter()\n",
    "    for j in range(i, i+d):\n",
    "        if j < n:\n",
    "            writer.add_page(pages[j])\n",
    "    os.makedirs(f'{LOCAL_INPUT_DIR}/{FILE_NAME}/PARTS/', exist_ok=True)\n",
    "    with open(f'{LOCAL_INPUT_DIR}/{FILE_NAME}/PARTS/{FILE_NAME}_{i+1}-{i+d}.pdf', 'wb') as f:\n",
    "        writer.write(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layout_to_text(layout: documentai.Document.Page.Layout, text: str) -> str:\n",
    "    \"\"\"\n",
    "    Document AI identifies text in different parts of the document by their\n",
    "    offsets in the entirety of the document's text. This function converts\n",
    "    offsets to a string.\n",
    "    \"\"\"\n",
    "    # If a text segment spans several lines, it will be stored in different text segments.\n",
    "    return ''.join(text[int(segment.start_index): int(segment.end_index)] for segment in layout.text_anchor.text_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_paths(dir_name: str) -> list:\n",
    "    file_paths = []\n",
    "    for file_name in os.listdir(dir_name):\n",
    "        if os.path.isfile(os.path.join(dir_name, file_name)):\n",
    "            file_path = os.path.join(dir_name, file_name)\n",
    "            file_paths.append(file_path)\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr_docai(file_path: str) -> dict:\n",
    "    pages_map = {}\n",
    "\n",
    "    with open(file_path, 'rb') as f:\n",
    "        pdf = f.read()\n",
    "        raw_document = documentai.RawDocument(content=pdf, mime_type='application/pdf')\n",
    "        request = documentai.ProcessRequest(name=DOCAI_PROCESSOR_NAME, raw_document=raw_document)\n",
    "        response = docai_client.process_document(request=request)\n",
    "        text = response.document.text\n",
    "        file_name = file_path.split('/')[-1]\n",
    "        page_number = int(file_name.split('.')[0].split('-')[-1])\n",
    "        for page in response.document.pages:\n",
    "            page_text = []\n",
    "            for paragraph in page.paragraphs:\n",
    "                paragraph_text = layout_to_text(paragraph.layout, text)\n",
    "                page_text.append(paragraph_text)\n",
    "            pages_map[page_number] = ''.join(page_text)\n",
    "            page_number += 1\n",
    "    return pages_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "input_dir = f'./DATA/INPUT/{FILE_NAME}/PARTS/'\n",
    "file_paths = get_file_paths(input_dir)\n",
    "    \n",
    "pages_map_list = []\n",
    "with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:  \n",
    "    pages_map_list = list(tqdm(executor.map(ocr_docai, file_paths)))\n",
    "\n",
    "merged_dict = {k: v for d in pages_map_list for k, v in d.items()}   \n",
    "sorted_pages_map = dict(sorted(merged_dict.items()))\n",
    "\n",
    "pages = []\n",
    "for _, page_text in sorted_pages_map.items():\n",
    "    pages.append(page_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save concatenated pages as txt for later use (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_pages = ''.join(pages)\n",
    "with open(f'{LOCAL_OUTPUT_DIR}/{FILE_NAME}/{FILE_NAME}.txt', 'wb') as out:\n",
    "    out.write(extracted_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_tokens(contexts: list) -> int:\n",
    "    total_tokens = 0\n",
    "    for context in contexts:\n",
    "        n_tokens = len(encoder.encode(context))\n",
    "        total_tokens += n_tokens \n",
    "    return total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tokens = get_total_tokens([extracted_pages])\n",
    "logger.info(f'Total tokens in the input doc = {total_tokens}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_tokens_per_page(contexts: list) -> list:\n",
    "    max_tokens_per_page = 0\n",
    "    for context in contexts:\n",
    "        n_tokens = len(encoder.encode(context))\n",
    "        if n_tokens > max_tokens_per_page:\n",
    "            max_tokens_per_page = n_tokens\n",
    "    return max_tokens_per_page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map Reduce 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_via_streaming_api(chunk: str) -> str:\n",
    "    access_token = !gcloud auth print-access-token\n",
    "    prompt = f'You are a Financial Regulations & Derivatives Expert. Summarize the following information into five brief sentences in English, capturing the essential details.\\n\\n{chunk}'\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {access_token[0]}\",\n",
    "        \"Content-Type\": \"application/json; charset=utf-8\"\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"inputs\": [\n",
    "            {\n",
    "                \"struct_val\": {\n",
    "                    \"prompt\": {\n",
    "                        \"string_val\": [prompt]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"parameters\": {\n",
    "            \"struct_val\": {\n",
    "                \"temperature\": {\"float_val\": 0.0},\n",
    "                \"maxOutputTokens\": {\"int_val\": 256},\n",
    "                \"topK\": {\"int_val\": 40},\n",
    "                \"topP\": {\"float_val\": 0.8}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    response = requests.post(STREAMING_API_URL, headers=headers, json=data)\n",
    "    content = json.loads(response.content)\n",
    "    output = []\n",
    "\n",
    "    for item in content:\n",
    "        try:\n",
    "            text = item['outputs'][0]['structVal']['content']['stringVal'][0]\n",
    "            output.append(text)\n",
    "        except Exception as e:\n",
    "            logger.error(f'Content error => {content}')\n",
    "    output = ''.join(output)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "CONTEXTS_PER_CALL = 5  # process 5 pages per API call\n",
    "MAX_OUTPUT_TOKENS = 256\n",
    "\n",
    "def reduce(contexts: list) -> list:\n",
    "    partitions = []\n",
    "    max_input_tokens = CONTEXT_LENGTH - MAX_OUTPUT_TOKENS\n",
    "    logger.info(f'Max input tokens = {max_input_tokens}')\n",
    "    max_tokens_per_page = get_max_tokens_per_page(contexts)\n",
    "    logger.info(f'Max tokens = {max_tokens_per_page}')\n",
    "    logger.info(f'Processing {CONTEXTS_PER_CALL} contexts per API call')\n",
    "    \n",
    "    for i in range(0, len(contexts), CONTEXTS_PER_CALL):\n",
    "        partitions.append(contexts[i: i+CONTEXTS_PER_CALL])\n",
    "\n",
    "    chunks = []\n",
    "    for partition in partitions:\n",
    "        chunks.append('\\n'.join(partition))\n",
    "\n",
    "    reduced_contexts = []\n",
    "\n",
    "    # max_workers can result in running over quota limits for invocation | current limit for text bison is 60/min\n",
    "    # for our experiments, we set max_workers=4 cores without any limit breach\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:  \n",
    "        reduced_contexts = list(tqdm(executor.map(get_summary_via_streaming_api, chunks),  total=len(chunks)))\n",
    "    return reduced_contexts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f'Number of pages to process = {len(pages)}')\n",
    "summaries = reduce(pages)\n",
    "logger.info(f'Number of generated summaries = {len(summaries)}')\n",
    "n_tokens = get_total_tokens(summaries)\n",
    "logger.info(f'Total number of tokens in generated summaries = {n_tokens}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(summaries[5])\n",
    "logger.info('-' * 100)\n",
    "logger.info(summaries[15])\n",
    "logger.info('-' * 100)\n",
    "logger.info(summaries[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map Reduce 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_via_streaming_api(context: str) -> str:\n",
    "    access_token = !gcloud auth print-access-token\n",
    "    prompt = f\"\"\"For the context below, create a consolidated refined short summary with the most important pointers only.\\n\\n{context}\\n\\nDo not repeat pointers. Breakdown the summary into SECTIONS. Make it crisp and concise.\"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {access_token[0]}\",\n",
    "        \"Content-Type\": \"application/json; charset=utf-8\"\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"inputs\": [\n",
    "            {\n",
    "                \"struct_val\": {\n",
    "                    \"prompt\": {\n",
    "                        \"string_val\": [prompt]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"parameters\": {\n",
    "            \"struct_val\": {\n",
    "                \"temperature\": {\"float_val\": 0.0},\n",
    "                \"maxOutputTokens\": {\"int_val\": 4096},\n",
    "                \"topK\": {\"int_val\": 40},\n",
    "                \"topP\": {\"float_val\": 0.8}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    response = requests.post(STREAMING_API_URL, headers=headers, json=data)\n",
    "    content = json.loads(response.content)\n",
    "    output = []\n",
    "\n",
    "    for item in content:\n",
    "        try:\n",
    "            text = item['outputs'][0]['structVal']['content']['stringVal'][0]\n",
    "            output.append(text)\n",
    "        except Exception as e:\n",
    "            logger.error(f'Content error => {content}')\n",
    "    output = ''.join(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "CONTEXTS_PER_CALL = 50  # process 50 summaries per API call\n",
    "\n",
    "def reduce(contexts: list) -> list:\n",
    "    partitions = []\n",
    "    logger.info(f'Processing {CONTEXTS_PER_CALL} contexts per API call')\n",
    "    \n",
    "    for i in range(0, len(contexts), CONTEXTS_PER_CALL):\n",
    "        partitions.append(contexts[i: i+CONTEXTS_PER_CALL])\n",
    "\n",
    "    chunks = []\n",
    "    for partition in partitions:\n",
    "        chunks.append('\\n'.join(partition))\n",
    "    logger.info(len(chunks))\n",
    "\n",
    "    reduced_contexts = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:  \n",
    "        reduced_contexts = list(tqdm(executor.map(get_summary_via_streaming_api, chunks),  total=len(chunks)))\n",
    "    return reduced_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_summaries = reduce(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f'Total number of summaries after map reduce 2 = {len(reduced_summaries)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(reduced_summaries[0])\n",
    "logger.info('-' * 100)\n",
    "logger.info(reduced_summaries[1])\n",
    "logger.info('-' * 100)\n",
    "logger.info(reduced_summaries[2])\n",
    "logger.info('-' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated_summaries = '\\n'.join(reduced_summaries)\n",
    "logger.info(get_total_tokens([consolidated_summaries]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(consolidated_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Consolidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_via_streaming_api(context: str) -> str:\n",
    "    access_token = !gcloud auth print-access-token\n",
    "    prompt = f\"\"\"Given the context below, combine and merge duplicate sections and pointers.\\n\\n{context}\\nAdd SECTIONS and bullets wherever needed. Clean rewrite and re-number sections.\"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {access_token[0]}\",\n",
    "        \"Content-Type\": \"application/json; charset=utf-8\"\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"inputs\": [\n",
    "            {\n",
    "                \"struct_val\": {\n",
    "                    \"prompt\": {\n",
    "                        \"string_val\": [prompt]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"parameters\": {\n",
    "            \"struct_val\": {\n",
    "                \"temperature\": {\"float_val\": 0.0},\n",
    "                \"maxOutputTokens\": {\"int_val\": 8192},\n",
    "                \"topK\": {\"int_val\": 40},\n",
    "                \"topP\": {\"float_val\": 0.8}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    response = requests.post(STREAMING_API_URL, headers=headers, json=data)\n",
    "    content = json.loads(response.content)\n",
    "    output = []\n",
    "\n",
    "    for item in content:\n",
    "        try:\n",
    "            text = item['outputs'][0]['structVal']['content']['stringVal'][0]\n",
    "            output.append(text)\n",
    "        except Exception as e:\n",
    "            logger.error(f'Content error => {content}')\n",
    "    output = ''.join(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_summary = get_summary_via_streaming_api(consolidated_summaries)\n",
    "logger.info(final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
