{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Form Summarization using `ChatGPT` (16k) and `DocAI`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import SystemMessagePromptTemplate\n",
    "from langchain.prompts.chat import HumanMessagePromptTemplate\n",
    "from google.api_core.client_options import ClientOptions\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.schema import HumanMessage\n",
    "from google.cloud import documentai\n",
    "from pypdf import PdfWriter\n",
    "from pypdf import PdfReader \n",
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "import vertexai\n",
    "import requests\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "import yaml\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup logging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Essentials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = './../credentials/vai-key.json'\n",
    "access_token = !gcloud auth print-access-token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./../credentials/oai-key.yml', 'rb') as f:\n",
    "    credentials = yaml.safe_load(f)\n",
    "    \n",
    "api_key = credentials['key']\n",
    "os.environ['OPENAI_API_KEY'] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'arun-genai-bb'\n",
    "LOCATION = 'us-central1'\n",
    "MODEL_NAME = 'gpt-3.5-turbo-16k'\n",
    "ENCODING_NAME = 'cl100k_base'\n",
    "CONTEXT_LENGTH = 16385  # GPT-4\n",
    "STREAMING_API_URL = f'https://us-central1-aiplatform.googleapis.com/ui/projects/{PROJECT_ID}/locations/us-central1/publishers/google/models/{MODEL_NAME}:serverStreamingPredict'\n",
    "DOCAI_PROCESSOR_NAME = 'projects/390991481152/locations/us/processors/ad9557a5be49204e'  # copy from notebook 00\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_options = ClientOptions(api_endpoint=f'us-documentai.googleapis.com')\n",
    "docai_client = documentai.DocumentProcessorServiceClient(client_options=client_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tiktoken.get_encoding(ENCODING_NAME)\n",
    "logger.info(f'Using encoder=={encoder.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(openai_api_key=api_key, \n",
    "                   model_name=MODEL_NAME, \n",
    "                   temperature=0.0, \n",
    "                   max_tokens=512)\n",
    "logger.info(f'Using model=={model.model_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Google DocumentAI to process input PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Break PDF into smaller PDFs for OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_INPUT_DIR = './DATA/INPUT'\n",
    "LOCAL_OUTPUT_DIR = './DATA/OUTPUT'\n",
    "FILE_NAME = 'file-2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(f'{LOCAL_INPUT_DIR}/{FILE_NAME}.pdf') \n",
    "pages = {}\n",
    "\n",
    "for i, page in enumerate(reader.pages):\n",
    "    pages[i] = page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(reader.pages)\n",
    "d = 15  # docai has a current constraint of 15 pages per document \n",
    "for i in range(0, n, d):\n",
    "    writer = PdfWriter()\n",
    "    for j in range(i, i+d):\n",
    "        if j < n:\n",
    "            writer.add_page(pages[j])\n",
    "    os.makedirs(f'{LOCAL_INPUT_DIR}/{FILE_NAME}/PARTS/', exist_ok=True)\n",
    "    with open(f'{LOCAL_INPUT_DIR}/{FILE_NAME}/PARTS/{FILE_NAME}_{i+1}-{i+d}.pdf', 'wb') as f:\n",
    "        writer.write(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layout_to_text(layout: documentai.Document.Page.Layout, text: str) -> str:\n",
    "    \"\"\"\n",
    "    Document AI identifies text in different parts of the document by their\n",
    "    offsets in the entirety of the document's text. This function converts\n",
    "    offsets to a string.\n",
    "    \"\"\"\n",
    "    # If a text segment spans several lines, it will be stored in different text segments.\n",
    "    return ''.join(text[int(segment.start_index): int(segment.end_index)] for segment in layout.text_anchor.text_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_paths(dir_name: str) -> list:\n",
    "    file_paths = []\n",
    "    for file_name in os.listdir(dir_name):\n",
    "        if os.path.isfile(os.path.join(dir_name, file_name)):\n",
    "            file_path = os.path.join(dir_name, file_name)\n",
    "            file_paths.append(file_path)\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr_docai(file_path: str) -> dict:\n",
    "    pages_map = {}\n",
    "\n",
    "    with open(file_path, 'rb') as f:\n",
    "        pdf = f.read()\n",
    "        raw_document = documentai.RawDocument(content=pdf, mime_type='application/pdf')\n",
    "        request = documentai.ProcessRequest(name=DOCAI_PROCESSOR_NAME, raw_document=raw_document)\n",
    "        response = docai_client.process_document(request=request)\n",
    "        text = response.document.text\n",
    "        file_name = file_path.split('/')[-1]\n",
    "        page_number = int(file_name.split('.')[0].split('-')[-1])\n",
    "        for page in response.document.pages:\n",
    "            page_text = []\n",
    "            for paragraph in page.paragraphs:\n",
    "                paragraph_text = layout_to_text(paragraph.layout, text)\n",
    "                page_text.append(paragraph_text)\n",
    "            pages_map[page_number] = ''.join(page_text)\n",
    "            page_number += 1\n",
    "    return pages_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "input_dir = f'./DATA/INPUT/{FILE_NAME}/PARTS/'\n",
    "file_paths = get_file_paths(input_dir)\n",
    "    \n",
    "pages_map_list = []\n",
    "with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:  \n",
    "    pages_map_list = list(tqdm(executor.map(ocr_docai, file_paths)))\n",
    "\n",
    "merged_dict = {k: v for d in pages_map_list for k, v in d.items()}   \n",
    "sorted_pages_map = dict(sorted(merged_dict.items()))\n",
    "\n",
    "pages = []\n",
    "for _, page_text in sorted_pages_map.items():\n",
    "    pages.append(page_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save concatenated pages as txt for later use (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_pages = ''.join(pages)\n",
    "os.makedirs(f'{LOCAL_OUTPUT_DIR}/{FILE_NAME}/OAI/3.5', exist_ok=True)\n",
    "with open(f'{LOCAL_OUTPUT_DIR}/{FILE_NAME}/OAI/3.5/{FILE_NAME}.txt', 'w') as out:\n",
    "    out.write(extracted_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_tokens(contexts: list) -> int:\n",
    "    total_tokens = 0\n",
    "    for context in contexts:\n",
    "        n_tokens = len(encoder.encode(context))\n",
    "        total_tokens += n_tokens \n",
    "    return total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tokens = get_total_tokens([extracted_pages])\n",
    "logger.info(f'Total tokens in the input doc = {total_tokens}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_tokens_per_page(contexts: list) -> list:\n",
    "    max_tokens_per_page = 0\n",
    "    for context in contexts:\n",
    "        n_tokens = len(encoder.encode(context))\n",
    "        if n_tokens > max_tokens_per_page:\n",
    "            max_tokens_per_page = n_tokens\n",
    "    return max_tokens_per_page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map Reduce 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(openai_api_key=api_key, \n",
    "                   model_name=MODEL_NAME, \n",
    "                   temperature=0.0, \n",
    "                   max_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(chunk: str) -> str:\n",
    "    template = \"You are a Financial Regulations & Derivatives Expert\"\n",
    "    system_message = SystemMessagePromptTemplate.from_template(template)\n",
    "\n",
    "    template = \"Summarize the following information into five brief sentences in English, capturing the essential details.\\n\\n{chunk}\"\n",
    "    human_message = HumanMessagePromptTemplate.from_template(template)\n",
    "\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message, human_message])\n",
    "    prompt = chat_prompt.format_prompt(chunk=chunk).to_messages()\n",
    "\n",
    "    response = model(prompt)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "MAX_OUTPUT_TOKENS = 256\n",
    "CONTEXTS_PER_CALL = 5  # process 5 pages per API call\n",
    "\n",
    "\n",
    "def reduce(contexts: list) -> list:\n",
    "    partitions = []\n",
    "    max_input_tokens = CONTEXT_LENGTH - MAX_OUTPUT_TOKENS\n",
    "    logger.info(f'Max input tokens allowed per API call = {max_input_tokens}')\n",
    "    max_tokens_per_page = get_max_tokens_per_page(contexts)\n",
    "    logger.info(f'Max tokens per page = {max_tokens_per_page}')\n",
    "    logger.info(f'Processing {CONTEXTS_PER_CALL} pages per API call')\n",
    "    \n",
    "    for i in range(0, len(contexts), CONTEXTS_PER_CALL):\n",
    "        partitions.append(contexts[i: i+CONTEXTS_PER_CALL])\n",
    "\n",
    "    chunks = []\n",
    "    for partition in partitions:\n",
    "        chunks.append('\\n'.join(partition))\n",
    "\n",
    "    reduced_contexts = []\n",
    "\n",
    "    # max_workers can result in running over quota limits for invocation | current limit for text bison is 60/min\n",
    "    # for our experiments, we set max_workers=4 cores without any limit breach\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:  \n",
    "        reduced_contexts = list(tqdm(executor.map(get_summary, chunks),  total=len(chunks)))\n",
    "    return reduced_contexts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f'Number of pages to process = {len(pages)}')\n",
    "summaries = reduce(pages)\n",
    "logger.info(f'Number of generated summaries = {len(summaries)}')\n",
    "n_tokens = get_total_tokens(summaries)\n",
    "logger.info(f'Total number of tokens in generated summaries = {n_tokens}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(summaries[5])\n",
    "logger.info('-' * 100)\n",
    "logger.info(summaries[15])\n",
    "logger.info('-' * 100)\n",
    "logger.info(summaries[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Persist internediate summaries (Map Reduce 1) to local disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f'Total number of summaries = {len(summaries)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, summary in enumerate(summaries):\n",
    "    os.makedirs(f'{LOCAL_OUTPUT_DIR}/{FILE_NAME}/OAI/3.5/MAP_REDUCE_1/', exist_ok=True)\n",
    "    with open(f'{LOCAL_OUTPUT_DIR}/{FILE_NAME}/OAI/3.5/MAP_REDUCE_1/summary-{i}.txt', 'w') as f:\n",
    "        f.write(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map Reduce 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(openai_api_key=api_key, \n",
    "                   model_name=MODEL_NAME, \n",
    "                   temperature=0.0, \n",
    "                   max_tokens=2048)\n",
    "\n",
    "def get_summary(context: str) -> str:\n",
    "    template = \"You are a Financial Regulations & Derivatives Expert\"\n",
    "    system_message = SystemMessagePromptTemplate.from_template(template)\n",
    "\n",
    "    template = \"\"\"\"For the context below, create a consolidated refined short summary with the most important pointers only.\\n\\n{context}\\n\\nDo not repeat pointers. Breakdown the summary into SECTIONS. Make it crisp and concise.\"\"\"\n",
    "    human_message = HumanMessagePromptTemplate.from_template(template)\n",
    "\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message, human_message])\n",
    "    prompt = chat_prompt.format_prompt(context=context).to_messages()\n",
    "\n",
    "    response = model(prompt)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "CONTEXTS_PER_CALL = 25  # process 50 summaries per API call\n",
    "\n",
    "def reduce(contexts: list) -> list:\n",
    "    partitions = []\n",
    "    max_input_tokens = CONTEXT_LENGTH - MAX_OUTPUT_TOKENS\n",
    "    logger.info(f'Max input tokens allowed per API call = {max_input_tokens}')\n",
    "    max_tokens_per_page = get_max_tokens_per_page(contexts)\n",
    "    logger.info(f'Max tokens per page = {max_tokens_per_page}')\n",
    "    logger.info(f'Processing {CONTEXTS_PER_CALL} pages per API call')\n",
    "    \n",
    "    for i in range(0, len(contexts), CONTEXTS_PER_CALL):\n",
    "        partitions.append(contexts[i: i+CONTEXTS_PER_CALL])\n",
    "\n",
    "    chunks = []\n",
    "    for partition in partitions:\n",
    "        chunks.append('\\n'.join(partition))\n",
    "    logger.info(f'Total number of chunks of summaries = {len(chunks)}')\n",
    "\n",
    "    reduced_contexts = []\n",
    "    for chunk in tqdm(chunks):\n",
    "        summary = get_summary(chunk)\n",
    "        reduced_contexts.append(summary)\n",
    "        time.sleep(1)\n",
    "    return reduced_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_summaries = reduce(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(reduced_summaries[0])\n",
    "logger.info('-' * 100)\n",
    "logger.info(reduced_summaries[1])\n",
    "logger.info('-' * 100)\n",
    "logger.info(reduced_summaries[2])\n",
    "logger.info('-' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Persist internediate summaries (Map Reduce 2) to local disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f'Total number of summaries after map reduce 2 = {len(reduced_summaries)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, summary in enumerate(reduced_summaries):\n",
    "    os.makedirs(f'{LOCAL_OUTPUT_DIR}/{FILE_NAME}/OAI/3.5/MAP_REDUCE_2/', exist_ok=True)\n",
    "    with open(f'{LOCAL_OUTPUT_DIR}/{FILE_NAME}/OAI/3.5/MAP_REDUCE_2/summary-{i}.txt', 'w') as f:\n",
    "        f.write(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated_summaries = '\\n'.join(reduced_summaries)\n",
    "logger.info(get_total_tokens([consolidated_summaries]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(consolidated_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Consolidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(openai_api_key=api_key, \n",
    "                   model_name=MODEL_NAME, \n",
    "                   temperature=0.0, \n",
    "                   max_tokens=1536)\n",
    "\n",
    "def get_summary(context: str) -> str:\n",
    "    template = \"You are a Financial Regulations & Derivatives Expert\"\n",
    "    system_message = SystemMessagePromptTemplate.from_template(template)\n",
    "\n",
    "    template = \"\"\"\"Given the context below, combine and merge duplicate sections and pointers.\\n\\n{context}\\nAdd SECTIONS and bullets wherever needed. Clean rewrite and re-number sections.\"\"\"\n",
    "    human_message = HumanMessagePromptTemplate.from_template(template)\n",
    "\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message, human_message])\n",
    "    prompt = chat_prompt.format_prompt(context=context).to_messages()\n",
    "\n",
    "    response = model(prompt)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_summary = get_summary(consolidated_summaries)\n",
    "logger.info(final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_OUTPUT_TOKENS = 4096\n",
    "max_input_tokens = CONTEXT_LENGTH - MAX_OUTPUT_TOKENS\n",
    "logger.info(f'Max input tokens allowed per API call = {max_input_tokens}')\n",
    "logger.info(f'Total tokens in final summary = {get_total_tokens([final_summary])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Persist final summary to local disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{LOCAL_OUTPUT_DIR}/{FILE_NAME}/OAI/3.5/final-summary.txt', 'w') as f:\n",
    "    f.write(final_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a filtered summary with all the proposed changes on the `Processing of Derivative Contracts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(openai_api_key=api_key, \n",
    "                   model_name=MODEL_NAME, \n",
    "                   temperature=0.0, \n",
    "                   max_tokens=MAX_OUTPUT_TOKENS)\n",
    "\n",
    "def get_summary(context: str) -> str:\n",
    "    template = \"You are a Financial Regulations & Derivatives Expert\"\n",
    "    system_message = SystemMessagePromptTemplate.from_template(template)\n",
    "\n",
    "    template = \"\"\"\"Given the SUMMARY below, extract and refine all proposed changes related to the processing of derivative contracts into a separate list. Create a concise and detailed summary with clear pointers.\\n\\n{context}\"\"\"\n",
    "    human_message = HumanMessagePromptTemplate.from_template(template)\n",
    "\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message, human_message])\n",
    "    prompt = chat_prompt.format_prompt(context=context).to_messages()\n",
    "\n",
    "    response = model(prompt)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposed_changes_summary = get_summary(consolidated_summaries)\n",
    "logger.info(proposed_changes_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_OUTPUT_TOKENS = 1536\n",
    "max_input_tokens = CONTEXT_LENGTH - MAX_OUTPUT_TOKENS\n",
    "logger.info(f'Max input tokens allowed per API call = {max_input_tokens}')\n",
    "logger.info(f'Total tokens in final summary = {get_total_tokens([proposed_changes_summary])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{LOCAL_OUTPUT_DIR}/{FILE_NAME}/OAI/3.5/proposed-changes-summary.txt', 'w') as f:\n",
    "    f.write(proposed_changes_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
